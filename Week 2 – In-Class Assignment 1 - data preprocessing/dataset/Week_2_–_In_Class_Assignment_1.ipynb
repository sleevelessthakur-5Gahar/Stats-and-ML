{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPAbWdMzsn-q"
      },
      "outputs": [],
      "source": [
        "#Please note that for each line of code, the explanation comment is inserted below the said line of code. (ALWAYS)\n",
        "#What shall be achieved:-\n",
        "#Data set --> CLEANING --> NOISE HANDLED --> OUTLIER HANDLING --> TRANSFORMED --> SCALING\n",
        "#############################################################################\n",
        "#STEP 1:-\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "#Telling pandas to ignore the commentes rows in the dataset also I used the following line of code because I was initially working on pycharm:-\n",
        "data=pd.read_csv(\"PS_2026.01.18_00.01.05.csv\",comment=\"#\",low_memory=False)\n",
        "#low_memory=False is used for avoiding mixed-type warnings\n",
        "#Reading the csv file that I downloaded from NASA.\n",
        "print(\"Output for data.shape :- \")\n",
        "print(data.shape)\n",
        "print()\n",
        "print()\n",
        "# This is an attribute, not a function (so you don't use parentheses ()).\n",
        "# It returns a tuple representing the dimensions of the DataFrame.\n",
        "# What it shows: (number_of_rows, number_of_columns).\n",
        "# Best for: Quick size checks.\n",
        "# If you perform a \"merge\" or \"drop\" operation, checking the .shape before and after is the fastest way to see if you lost or gained data unexpectedly.\n",
        "print(\"Output for data.head(10) :-\")\n",
        "print(data.head(10))\n",
        "print()\n",
        "print()\n",
        "print(\"Output for data.tail(20) :- \")\n",
        "print(data.tail(20))\n",
        "# This returns the first n rows of the DataFrame (the default is 5).\n",
        "# What it shows: An actual \"snapshot\" of your raw data.Best for: Getting a visual sense of the data.\n",
        "# It helps you see if the headers are aligned correctly, if there are weird characters in the strings,\n",
        "# or if the data formatting looks consistent.Note: You can also use df.tail() to see the last few rows which I have.\n",
        "print()\n",
        "print()\n",
        "print(\"Output for data.info() :- \")\n",
        "print(data.info())\n",
        "# This method provides a high-level summary of the DataFrame's structure. It is the best tool for checking if your data loaded correctly.\n",
        "# What it shows: * The total number of rows (entries) and columns.\n",
        "# The name of every column.\n",
        "# The number of non-null (not missing) values in each column.\n",
        "# The Data Type (e.g., int64, float64, object for strings).\n",
        "# How much memory (RAM) the DataFrame is using.\n",
        "# Best for: Identifying missing values and checking if column types are correct (e.g., ensuring a \"Date\" column isn't being read as a \"String\"):-\n",
        "print()\n",
        "print()\n",
        "print(\"Output for data.describe() :- \")\n",
        "print(data.describe())\n",
        "# This generates descriptive statistics for the numerical columns in your dataset.\n",
        "# What it shows: * Count: Number of non-empty values.\n",
        "# Mean: The average value.\n",
        "# Std: Standard deviation (how spread out the data is).\n",
        "# Min/Max: The smallest and largest values.\n",
        "# 25%, 50%, 75%: Percentiles (the 50% mark is the Median).\n",
        "# Best for: Detecting outliers. For example, if you are looking at \"Planet Radius\" and the max is 1,000\n",
        "# times larger than the mean, you likely have an outlier or a data entry error.\n",
        "print()\n",
        "print()\n",
        "print()\n",
        "print()\n",
        "columns_to_keep = [\n",
        "    # Star identifiers & position\n",
        "    \"hostname\", \"ra\", \"dec\",\n",
        "\n",
        "    # Stellar properties\n",
        "    \"st_spectype\", \"st_mass\", \"st_rad\", \"st_teff\", \"st_lum\",\n",
        "    \"st_logg\", \"st_met\", \"st_age\", \"sy_dist\",\n",
        "\n",
        "    # Planet identifiers\n",
        "    \"pl_name\",\n",
        "\n",
        "    # Planet physical properties\n",
        "    \"pl_rade\", \"pl_masse\", \"pl_dens\", \"pl_eqt\", \"pl_insol\",\n",
        "    \"pl_bmasse\", \"pl_orbeccen\", \"pl_orbincl\", \"pl_ratror\",\n",
        "    \"pl_trandep\", \"pl_trandur\",\n",
        "\n",
        "    # Orbital properties\n",
        "    \"pl_orbper\", \"pl_orbsmax\", \"pl_orbtper\", \"pl_orblper\",\n",
        "    \"pl_ratdor\", \"pl_imppar\", \"pl_tranmid\", \"pl_orbeccenlim\",\n",
        "\n",
        "    # Discovery & observation metadata\n",
        "    \"discoverymethod\", \"disc_year\", \"disc_facility\",\n",
        "    \"disc_telescope\", \"disc_instrument\",\n",
        "    \"rv_flag\", \"tran_flag\", \"ttv_flag\"\n",
        "]\n",
        "data = data[columns_to_keep]\n",
        "#This is where we delete the unnecessary columns and keep only what we need (40 columns)\n",
        "# data[---] = select specific columns\n",
        "# columns_to_keep = the 40 columns we care about\n",
        "# data =  overwrite the dataframe with the reduced version\n",
        "print(\"Remaining columns:\", data.shape[1])\n",
        "\n",
        "newNames = {\n",
        "    # Star identifiers & position\n",
        "    \"hostname\": \"host_star_name\",\n",
        "    \"ra\": \"right_ascension_deg\",\n",
        "    \"dec\": \"declination_deg\",\n",
        "\n",
        "    # Stellar properties\n",
        "    \"st_spectype\": \"star_spectral_type\",\n",
        "    \"st_mass\": \"star_mass_solar\",\n",
        "    \"st_rad\": \"star_radius_solar\",\n",
        "    \"st_teff\": \"star_temperature_K\",\n",
        "    \"st_lum\": \"star_luminosity_solar\",\n",
        "    \"st_logg\": \"star_surface_gravity_log\",\n",
        "    \"st_met\": \"star_metallicity\",\n",
        "    \"st_age\": \"star_age_gyr\",\n",
        "    \"sy_dist\": \"star_distance_parsec\",\n",
        "\n",
        "    # Planet identifiers\n",
        "    \"pl_name\": \"planet_name\",\n",
        "\n",
        "    # Planet physical properties\n",
        "    \"pl_rade\": \"planet_radius_earth\",\n",
        "    \"pl_masse\": \"planet_mass_earth\",\n",
        "    \"pl_dens\": \"planet_density\",\n",
        "    \"pl_eqt\": \"equilibrium_temperature_K\",\n",
        "    \"pl_insol\": \"stellar_irradiance_earth_units\",\n",
        "    \"pl_bmasse\": \"best_planet_mass_earth\",\n",
        "    \"pl_orbeccen\": \"orbital_eccentricity\",\n",
        "    \"pl_orbincl\": \"orbital_inclination_deg\",\n",
        "    \"pl_ratror\": \"planet_star_radius_ratio\",\n",
        "    \"pl_trandep\": \"transit_depth\",\n",
        "    \"pl_trandur\": \"transit_duration_hours\",\n",
        "\n",
        "    # Orbital properties\n",
        "    \"pl_orbper\": \"orbital_period_days\",\n",
        "    \"pl_orbsmax\": \"semi_major_axis_AU\",\n",
        "    \"pl_orbtper\": \"time_of_periastron_days\",\n",
        "    \"pl_orblper\": \"longitude_of_periastron_deg\",\n",
        "    \"pl_ratdor\": \"distance_to_star_radius_ratio\",\n",
        "    \"pl_imppar\": \"impact_parameter\",\n",
        "    \"pl_tranmid\": \"transit_midpoint_time\",\n",
        "    \"pl_orbeccenlim\": \"orbital_eccentricity_limit_flag\",\n",
        "\n",
        "    # Discovery & observation metadata\n",
        "    \"discoverymethod\": \"discovery_method\",\n",
        "    \"disc_year\": \"discovery_year\",\n",
        "    \"disc_facility\": \"discovery_facility\",\n",
        "    \"disc_telescope\": \"discovery_telescope\",\n",
        "    \"disc_instrument\": \"discovery_instrument\",\n",
        "    \"rv_flag\": \"radial_velocity_used\",\n",
        "    \"tran_flag\": \"transit_method_used\",\n",
        "    \"ttv_flag\": \"transit_timing_variation_used\"\n",
        "}\n",
        "#Pretty self explanatory that we are renaming all of the columns which will help the stakeholders and me\n",
        "#Me specifically during the feature engineering phase.\n",
        "#We have made a dictionary in which:-\n",
        "#Keys are old NASA column names\n",
        "#Values are clean scientific names\n",
        "data=data.rename(columns=newNames)\n",
        "#Applying the renaming.\n",
        "#This step doesn't have to happen now but I did it just in case.\n",
        "print()\n",
        "print()\n",
        "print()\n",
        "print(\"Column names old and new in the dataset:- \")\n",
        "print(data.columns.tolist())\n",
        "#The aforementioned line of code will display each column as a list.\n",
        "#I personally wanted for it to be like this because of renaming that I did.\n",
        "print()\n",
        "print()\n",
        "print()\n",
        "print(\"Number of missing values in each column:- \")\n",
        "pd.set_option('display.max_rows', None)\n",
        "#The aforementioned line of code will set display rule to none.\n",
        "#Meanning everything will be displayed.\n",
        "print(data.isnull().sum())\n",
        "#What data.isNull() do:-\n",
        "#This creates a new table (DataFrame) of the same size as df.\n",
        "# Every value becomes:\n",
        "# True = if the original value is missing\n",
        "# False = if the original value exists\n",
        "#What .sum() does:-\n",
        "#In computer:-\n",
        "#True = 1\n",
        "# False = 0\n",
        "# So .sum():\n",
        "# Adds up all True values column by column\n",
        "# Which results in:\n",
        "# Number of missing values per column\n",
        "pd.reset_option('display.max_rows')\n",
        "#The aforementioned line of code will reset the rule to normal so that in future when we try displaying a DataFrame,\n",
        "# everything is not displayed.\n",
        "###################################################################################\n",
        "#We are now all set for:-\n",
        "# Missing value handling 1\n",
        "# Noise simulation 2\n",
        "# Outlier detection 3\n",
        "# Feature engineering 4\n",
        "# Encoding & scaling 5\n",
        "###################################################################################\n",
        "###################################################################################\n",
        "###################################################################################\n",
        "###################################################################################\n",
        "# STEP 1.1 Missing value handling :-\n",
        "# We now have:-\n",
        "# Renamed columns (good descriptive names like orbital_period_days)\n",
        "# Printed missing values per column\n",
        "# A very large real-world dataset (40k rows, 289 columns)\n",
        "####################################\n",
        "#We will use Median for numeric values because of outliers in the NASA dataset since we have planets/stars which can weigh up to\n",
        "#Million times the mass of our sun.\n",
        "#We will use Mode for Categorical values because we can't perform maths on words.\n",
        "#Missing Value Handling (Safe Approach):-\n",
        "numerical_columns = [\n",
        "    \"right_ascension_deg\",\n",
        "    \"declination_deg\",\n",
        "    \"star_mass_solar\",\n",
        "    \"star_radius_solar\",\n",
        "    \"star_temperature_K\",\n",
        "    \"star_age_gyr\",\n",
        "    \"star_distance_parsec\",\n",
        "    \"planet_radius_earth\",\n",
        "    \"orbital_period_days\",\n",
        "    \"semi_major_axis_AU\",\n",
        "    \"discovery_year\" ]\n",
        "#These are numeric columns where using median wonâ€™t break\n",
        "#ASTROPHYSICS principles too badly in general statistics.\n",
        "categorical_columns = [\n",
        "    \"host_star_name\",\n",
        "    \"planet_name\",\n",
        "    \"star_spectral_type\",\n",
        "    \"discovery_method\",\n",
        "    \"discovery_facility\",\n",
        "    \"discovery_telescope\",\n",
        "    \"discovery_instrument\",\n",
        "    \"radial_velocity_used\",\n",
        "    \"transit_method_used\",\n",
        "    \"transit_timing_variation_used\"\n",
        "]\n",
        "#These columns are categorical,\n",
        "# so replacing missing values with the mode (most common value) is safe.\n",
        "# Fill numerical columns with median\n",
        "physics_calculable_cols = [\n",
        "    \"star_luminosity_solar\",\n",
        "    \"planet_density\",\n",
        "    \"equilibrium_temperature_K\",\n",
        "    \"stellar_irradiance_earth_units\",\n",
        "    \"star_surface_gravity_log\",\n",
        "    \"planet_mass_earth\",\n",
        "    \"best_planet_mass_earth\",\n",
        "    \"orbital_eccentricity\",\n",
        "    \"orbital_inclination_deg\",\n",
        "    \"planet_star_radius_ratio\",\n",
        "    \"transit_depth\",\n",
        "    \"transit_duration_hours\",\n",
        "    \"time_of_periastron_days\",\n",
        "    \"longitude_of_periastron_deg\",\n",
        "    \"distance_to_star_radius_ratio\",\n",
        "    \"impact_parameter\",\n",
        "    \"transit_midpoint_time\",\n",
        "    \"orbital_eccentricity_limit_flag\",\n",
        "]\n",
        "#The aforementioned are calculable columns which we will calculate later during feature engineering.\n",
        "for i in numerical_columns:\n",
        "    median=data[i].median()\n",
        "    data[i]=data[i].fillna(median)\n",
        "\n",
        "for i in categorical_columns:\n",
        "    mode=data[i].mode()[0]\n",
        "    data[i]=data[i].fillna(mode)\n",
        "for i in physics_calculable_cols:\n",
        "    missingVals=data[i].isnull() #This will fetch all of the values which are null in the physics calculable col list.\n",
        "    #True = missing\n",
        "    #False = value exists\n",
        "    newColumn=i+\"_is_missing_values\"\n",
        "    data[newColumn]=missingVals\n",
        "print()\n",
        "print()\n",
        "print()\n",
        "print(\"Number of new missing vals:\")\n",
        "print(data.isnull().sum())\n",
        "#After this print, we will be able to see that only physics calculable columns\n",
        "#have missing values or NaN's\n",
        "#which is fine.\n",
        "#We have already filled in numerical and categorical columns which we could have.\n",
        "####################################\n",
        "####################################\n",
        "# Step 2:-\n",
        "#Visualize missing values in the dataset\n",
        "print()\n",
        "print()\n",
        "print()\n",
        "print()\n",
        "print(\"A quick check of categorical and numerical column NaNs\\n\")\n",
        "print(\"You will see that we have 0 for all which is the best thing\\n\")\n",
        "safe_columns = numerical_columns + categorical_columns\n",
        "print(data[safe_columns].isnull().sum())\n",
        "print()\n",
        "print()\n",
        "print()\n",
        "print()\n",
        "print(\"\\nA quick check for physics columns\\n\")\n",
        "print(\"We are allowed to have NaN's here:\\n\")\n",
        "print(data[physics_calculable_cols].isnull().sum())\n",
        "print()\n",
        "print()\n",
        "print()\n",
        "print()\n",
        "print(\"Verifying missing-value flags for physics calculable columns (True=missing, False=present):\\n\")\n",
        "flag_columns = []\n",
        "# flag_columns = [] ========>> We start with an empty list to store column names.\n",
        "# Loop through each physics-calculable column:-\n",
        "for i in physics_calculable_cols: #for col in physics_calculable_cols: ====== Go through each column that we plan to calculate using physics formulas.\n",
        "    flag_columns.append(i + \"_is_missing_values\")\n",
        "    # col + \"_is_missing_values\" =======>>\n",
        "    # Add a descriptive suffix to indicate this column tracks missing values.\n",
        "    #.append() ==== Add the new name to our flag_columns list.\n",
        "print(data[flag_columns].head(10))\n",
        "#The output is directly related to line 263 which is:-\n",
        "# missingVals=data[i].isnull()\n",
        "#Which will fetch all of the values which are null in the physics calculable col list.\n",
        "#True = missing\n",
        "#False = value exists\n",
        "# Display the first 10 rows of the missing-value flag columns\n",
        "print(\"\\n\\n\\n\\nCount of True values in missing-value flags (number of missing entries):\\n\")\n",
        "for i in flag_columns:\n",
        "    # .sum() counts the True values (missing entries)\n",
        "    print(i, \":\", data[i].sum())\n",
        "    #data[i].sum() :-\n",
        "    # Since True = 1 and False = 0,\n",
        "    #Basically .sum() here will keep adding 1\n",
        "    # .sum() gives the number of missing values in that column.\n",
        "#Step 1: Handling Missing Values\n",
        "#  We have identified columns with missing values using data.isnull().sum().\n",
        "#  We filled safe-to-fill numerical columns with the median.\n",
        "#  We filled safe-to-fill categorical columns with the mode.\n",
        "#  For physics-calculable columns, we left them as NaN and created\n",
        "#  True/False missing-value flag columns.\n",
        "#  We have verified that only physics-calculable columns have missing values left.\n",
        "# So Step 1 is basically complete â€” all safe-to-fill columns are cleaned,\n",
        "# and calculable columns are tracked with flags.\n",
        "###########################################################################\n",
        "###########################################################################\n",
        "###########################################################################\n",
        "###########################################################################\n",
        "#Step 2: Noise Detection and Handling\n",
        "# Select one numerical feature.\n",
        "# Add artificial noise (small random variations) to this feature.\n",
        "# Apply a simple noise-handling technique, such as:\n",
        "# Moving average\n",
        "# Smoothing by aggregation\n",
        "# Compare the feature before and after noise handling.\n",
        "# ðŸ“Œ Goal: Understand the difference between raw and cleaned signals.\n",
        "detector=\"star_mass_solar\"\n",
        "#For step 2 we need a numerical feature so we take the mass of suns in all of the universes\n",
        "#Think of it like: if a starâ€™s mass is 1.0 solar mass,\n",
        "# real-world measurements might be slightly 1.01 or 0.99 = this is noise.\n",
        "#######################################\n",
        "#ADDITION OF RANDOM ARTIFICIAL REPEATABLE NOISE:-\n",
        "np.random.seed(42)\n",
        "#This ensures the random numbers are always the same every time we run the code.\n",
        "# Otherwise, every run gives slightly different numbers.\n",
        "#Why 42? Because Hitchhikerâ€™s Guide to the Galaxy joke\n",
        "# (â€œAnswer to the Ultimate Question of Life, the Universe, and Everythingâ€).\n",
        "########################################\n",
        "#CREATION OF NOISE:-\n",
        "noiseBalance=0.05*data[detector].median()\n",
        "#We do a 5% of median of detector in dataFrame data which is star_mass_solar\n",
        "actualNoise=np.random.normal(loc=0,scale=noiseBalance,size=len(data))\n",
        "#np.random.normal(loc=0, scale=noise_strength, size=len(data)) will  generate random numbers such that:-\n",
        "#loc=0 ----> is the center of all.\n",
        "#Scale ---> Defines the spread in our case can be 0 to 1.5 or 0 to -1.5 keep in mind that only 68% of the times this rule will be followed.\n",
        "#Size beasically defines the amount of this has to be followed for.\n",
        "########################################\n",
        "#ADD THE NOISE TO THE ORIGINAL COLUMN AND MAKE A NEW COLUMN FOR REPRESENTATION:-\n",
        "data[detector+\"_Noisy_New\"]=data[detector]+actualNoise #adds the noise we just created to the original column\n",
        "#detector + \"_Nosy_New\" -> creates a new column name \"star_mass_solar_Noisy_New\"\n",
        "########################################\n",
        "#SMOOTHING THE NOISY DATA:-\n",
        "gaps=5\n",
        "data[detector+\"_SmoothValues\"]=data[detector+\"_Noisy_New\"].rolling(window=gaps,min_periods=1).mean()\n",
        "#The aforementioned technique is used from Dr Jalali's github repo: - Link :- https://prnt.sc/coYZpo_1-931\n",
        "#.rolling() â†’ creates a \"moving window\" of 5 rows\n",
        "# Example: if rows are [1,2,3,4,5,6,7] and window=3:\n",
        "# First window = [1] â†’ average = 1\n",
        "# Second window = [1,2] â†’ average = 1.5\n",
        "# Third window = [1,2,3] â†’ average = 2\n",
        "# Fourth window = [2,3,4] â†’ average = 3 and so on\n",
        "#More Example:-\n",
        "#Data: [10, 12, 15, 13, 14, 20, 18]\n",
        "# Window size: 3\n",
        "# Step 1: [10] -> mean = 10\n",
        "# Step 2: [10,12] -> mean = 11\n",
        "# Step 3: [10,12,15] -> mean = 12.33\n",
        "# Step 4: [12,15,13] -> mean = 13.33\n",
        "# Step 5: [15,13,14] -> mean = 14\n",
        "# Step 6: [13,14,20] -> mean = 15.67\n",
        "# Step 7: [14,20,18] -> mean = 17.33\n",
        "#After this, the noisy spikes will be reduced.\n",
        "########################################\n",
        "#COMPARISON B/W ORIGINAL AND NOISY:-\n",
        "# Print the first 10 rows of the original, noisy, and smoothed columns\n",
        "print(\"\\nStep 2 â€“ Noise Detection and Handling:\")\n",
        "print(f\"Feature selected: {detector}\\n\")\n",
        "print(data[[detector, detector + \"_Noisy_New\", detector + \"_SmoothValues\"]].head(10))\n",
        "###########################################################################\n",
        "###########################################################################\n",
        "###########################################################################\n",
        "###########################################################################\n",
        "#STEP 3:-\n",
        "#Outlier detection and handling:-\n",
        "#Outliers are extreme values in your dataset, very far from the majority of data points.\n",
        "# We want to detect them and decide whether to remove, cap, or leave them.\n",
        "#Most stars are around 1â€“2 solar masses, but for an example 23.5 is very extreme and real\n",
        "#In our case, the extreme values are real stars, so we will cap unrealistic numbers only, instead of removing them.\n",
        "# Z-Score Formula:- https://prnt.sc/e9ZM-TVc0sP7\n",
        "#Where:- https://prnt.sc/LISPdPljB8Hm\n",
        "#Example:-https://prnt.sc/5ZVBVsajjB5l\n",
        "#Takeaways:- https://prnt.sc/uEN_PNVkPR1q\n",
        "############################################\n",
        "#Calculating Z Scores:-\n",
        "from scipy.stats import zscore\n",
        "data['star_mass_solar_zscore']=zscore(data['star_mass_solar'].fillna(data['star_mass_solar'].median()))\n",
        "#zscore(--------) = calculates (X-mean)/std for every value/row\n",
        "#.fillna(data['star_mass_solar'].median()) ---> temporarily fills missing values with median to avoid errors\n",
        "#####################################################\n",
        "#Capping rules because extreme values in the data set are real\n",
        "#A star can weigh 23 timees that of our sun.\n",
        "#So, Maximum allowed = mean + 3*standar Deviation\n",
        "# Minimum allowed = mean â€“ 3*standard deviation\n",
        "# Calculating Limits:-\n",
        "#Stars are not evenly distributed.\n",
        "# So instead of mean/std, we use percentiles/quantiles in python.\n",
        "# Percentile means:\n",
        "# Bottom 1%\n",
        "# Top 1%\n",
        "upper_limit = data['star_mass_solar'].quantile(0.999)\n",
        "lower_limit = data['star_mass_solar'].quantile(0.001)\n",
        "\n",
        "#Copying:-\n",
        "data['star_mass_solar_capped'] = data['star_mass_solar']\n",
        "#Applying the capping using pandas:-\n",
        "data['star_mass_solar_capped'] = data['star_mass_solar_capped'].clip(lower=lower_limit,upper=upper_limit)\n",
        "#Counting the changes:-\n",
        "num_high = (data['star_mass_solar'] > upper_limit).sum()\n",
        "num_low  = (data['star_mass_solar'] < lower_limit).sum()\n",
        "print(\"High-end values capped:\", num_high)\n",
        "print(\"Low-end values capped:\", num_low)\n",
        "#############################\n",
        "#Using the log:-\n",
        "data['star_mass_solar_log'] = np.log10(data['star_mass_solar'])\n",
        "#Did this right now for future use when I start doing ML.\n",
        "\n",
        "print(\n",
        "    data[['star_mass_solar',\n",
        "          'star_mass_solar_zscore',\n",
        "          'star_mass_solar_capped',\n",
        "          'star_mass_solar_log']].head(10)\n",
        ")\n",
        "print(data[\"star_mass_solar_capped\"].describe())\n",
        "#######################################################################\n",
        "#######################################################################\n",
        "#######################################################################\n",
        "#######################################################################\n",
        "#Step 4:- Feature engineering\n",
        "# Step 4.1: Calculate stellar density (approximate)\n",
        "data['star_density_solar'] = data['star_mass_solar_capped'] / ((4/3) * np.pi * (data['star_radius_solar']**3))\n",
        "# Explanation:\n",
        "# star_mass_solar_capped -> mass of star in solar masses (after removing outliers)\n",
        "# star_radius_solar -> radius of star in solar radii\n",
        "# (4/3)*pi*R^3 -> volume of a sphere\n",
        "# Dividing mass by volume gives density\n",
        "###########################################################\n",
        "# Step 4.2: Calculate planet mass / star mass ratio\n",
        "data['planet_star_mass_ratio'] = data['planet_mass_earth'] / (data['star_mass_solar_capped'] * 332946)\n",
        "# Explanation:\n",
        "# planet_mass_earth -> planet mass in Earth masses\n",
        "# star_mass_solar_capped -> star mass in solar masses\n",
        "# 1 solar mass = 332,946 Earth masses\n",
        "# So we convert star mass to Earth masses to compute ratio\n",
        "###########################################################\n",
        "# Step 4.3: Log transform of stellar luminosity\n",
        "data['star_luminosity_solar_log'] = np.log10(data['star_luminosity_solar'].replace(0, np.nan))\n",
        "# Explanation:\n",
        "# replace(0, np.nan) avoids log(0) which is undefined\n",
        "# Log10 scales down very large luminosities\n",
        "###########################################################\n",
        "# Step 4.3a: Log transform of planet radius\n",
        "data['planet_radius_earth_log'] = np.log10(data['planet_radius_earth'].replace(0, np.nan) + 1)\n",
        "# Explanation:\n",
        "# replace 0 with NaN + 1 to avoid log(0) which is undefined\n",
        "# Log10 scales down very large planet radii for ML\n",
        "###########################################################\n",
        "# Step 4.4: Flag planets as rocky or gaseous based on density\n",
        "data['planet_type'] = np.where(data['planet_density'] > 5, 'rocky',\n",
        "                               np.where(data['planet_density'] < 1, 'gas_giant', 'intermediate'))\n",
        "# Explanation:\n",
        "# np.where(condition, value_if_true, value_if_false)\n",
        "# First condition: density > 5 -> 'rocky'\n",
        "# Else if density < 1 -> 'gas_giant'\n",
        "# Else -> 'intermediate'\n",
        "###########################################################\n",
        "# Step 4.5: Convert orbital period from days to years\n",
        "data['orbital_period_years'] = data['orbital_period_days'] / 365.25\n",
        "# Explanation:\n",
        "# orbital_period_days -> days\n",
        "# Divide by 365.25 to convert to years\n",
        "###########################################################\n",
        "# Step 4.6: Flag planets in habitable zone\n",
        "# Safely replace negative or zero luminosity with NaN before sqrt\n",
        "data['hz_inner'] = np.sqrt(data['star_luminosity_solar'].replace(0, np.nan) / 1.1)\n",
        "data['hz_outer'] = np.sqrt(data['star_luminosity_solar'].replace(0, np.nan) / 0.53)\n",
        "data['in_habitable_zone'] = np.where((data['semi_major_axis_AU'] >= data['hz_inner']) &\n",
        "                                     (data['semi_major_axis_AU'] <= data['hz_outer']), True, False)\n",
        "# Explanation:\n",
        "# hz_inner and hz_outer -> boundaries in AU\n",
        "# semi_major_axis_AU -> planet distance in AU\n",
        "# If planet distance is within boundaries -> True\n",
        "print(data[[\n",
        "    'star_mass_solar_capped',\n",
        "    'star_mass_solar_log',\n",
        "    'star_luminosity_solar',\n",
        "    'hz_inner',\n",
        "    'hz_outer',\n",
        "    'semi_major_axis_AU',\n",
        "    'in_habitable_zone',\n",
        "    'planet_radius_earth',\n",
        "    'planet_radius_earth_log'\n",
        "]].head(10))\n",
        "#######################################################################\n",
        "#######################################################################\n",
        "#######################################################################\n",
        "#######################################################################\n",
        "#STEP 5:- Scaling & Encoding\n",
        "# We now have a cleaned and feature-engineered dataset\n",
        "# Some features are numeric (continuous)\n",
        "###########################################################\n",
        "# Step 5.1: Scaling numeric columns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Select numeric columns for scaling\n",
        "numeric_features = [\n",
        "    \"right_ascension_deg\",\n",
        "    \"declination_deg\",\n",
        "    \"star_mass_solar_capped\",\n",
        "    \"star_radius_solar\",\n",
        "    \"star_temperature_K\",\n",
        "    \"star_age_gyr\",\n",
        "    \"star_distance_parsec\",\n",
        "    \"planet_radius_earth\",\n",
        "    \"planet_mass_earth\",\n",
        "    \"planet_density\",\n",
        "    \"planet_star_mass_ratio\",     # Derived numerical feature\n",
        "    \"planet_radius_earth_log\",\n",
        "    \"star_luminosity_solar_log\",\n",
        "    \"star_surface_gravity_log\",\n",
        "    \"orbital_period_days\",\n",
        "    \"orbital_period_years\",\n",
        "    \"semi_major_axis_AU\",\n",
        "    \"stellar_irradiance_earth_units\",\n",
        "    \"equilibrium_temperature_K\",\n",
        "    \"impact_parameter\",\n",
        "    \"distance_to_star_radius_ratio\",\n",
        "    \"transit_depth\",\n",
        "    \"transit_duration_hours\",\n",
        "    \"star_density_solar\"          # Derived numerical feature\n",
        "]\n",
        "# Explanation:\n",
        "# These are all numeric values which we want to scale for ML\n",
        "# Scaling ensures features are on similar scale, avoids domination by large values\n",
        "\n",
        "# Initialize StandardScaler\n",
        "scaler = StandardScaler()\n",
        "# Explanation:\n",
        "# StandardScaler subtracts mean and divides by std\n",
        "# Resulting distribution has mean=0, std=1\n",
        "\n",
        "# Apply scaling and overwrite columns\n",
        "data[numeric_features] = scaler.fit_transform(data[numeric_features])\n",
        "# Explanation:\n",
        "# fit_transform calculates mean/std from data\n",
        "# and scales each column automatically\n",
        "# Replaces original numeric values with scaled ones\n",
        "###########################################################\n",
        "# Step 5.2: Verify final ML-ready dataset\n",
        "print(\"Shape of final ML-ready dataset:\", data.shape)\n",
        "print(data.head(5))\n",
        "# Explanation:\n",
        "# Check number of rows and columns\n",
        "# Display first 5 rows to confirm everything is numeric and scaled\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}